{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.48, 'b': 0.52}\n"
     ]
    }
   ],
   "source": [
    "# Helper function. Adds vectors and b, with a coefficient (1-d) for a and d for b\n",
    "def add_vectors(vector_a, vector_b, ca, cb):\n",
    "    keys = set(vector_a.keys()) | set(vector_b.keys())\n",
    "    result = dict()\n",
    "    for c in keys:\n",
    "        p_a = vector_a.get(c)\n",
    "        if p_a == None:\n",
    "            p_a = 0.0\n",
    "        p_b = vector_b.get(c)\n",
    "        if p_b == None:\n",
    "            p_b = 0.0\n",
    "        result[c] = ca * p_a + cb * p_b\n",
    "    return result\n",
    "\n",
    "# test\n",
    "def test_add_vectors():\n",
    "    ta = {\"a\":0.6, \"b\":0.4}\n",
    "    tb = {\"b\":1.0}\n",
    "    print add_vectors(ta, tb, 0.8, 0.2)\n",
    "\n",
    "test_add_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.6, 'b': 0.6}\n"
     ]
    }
   ],
   "source": [
    "from math import fabs\n",
    "\n",
    "# Helper function. Computes the absolute difference between vectors and b\n",
    "# We use it to test convergence of pagerank computation\n",
    "def diff_vectors(vector_a, vector_b):\n",
    "    keys = set(vector_a.keys()) | set(vector_b.keys())\n",
    "    result = dict()\n",
    "    for c in keys:\n",
    "        p_a = vector_a.get(c)\n",
    "        if p_a == None:\n",
    "            p_a = 0.0\n",
    "        p_b = vector_b.get(c)\n",
    "        if p_b == None:\n",
    "            p_b = 0.0\n",
    "        result[c] = fabs(p_a - p_b)\n",
    "    return result\n",
    "\n",
    "# test\n",
    "def test_diff_vectors():\n",
    "    ta = {\"a\":0.6, \"b\":0.4}\n",
    "    tb = {\"b\":1.0}\n",
    "    print diff_vectors(ta, tb)\n",
    "\n",
    "test_diff_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0.6, 'b': 0.4}\n"
     ]
    }
   ],
   "source": [
    "# Helper function. Takes as input a vector/dictionary, where we have nodes and counts\n",
    "# and returns a probability vector\n",
    "def normalize_vector(vector):\n",
    "    norm = sum(vector.values())\n",
    "    return { v: 1.0*vector[v]/norm for v in vector}\n",
    "\n",
    "# test\n",
    "def test_normalize_vector():\n",
    "    ta = {\"a\":6, \"b\":4}\n",
    "    print normalize_vector(ta)\n",
    "    \n",
    "test_normalize_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 0.2, 'b': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# Helper function. Takes as input a vector/dictionary, where we have nodes and probabilities\n",
    "# (Implicit assumption that the probabilities add up to 1.0)\n",
    "# Then removes the specified node, and renormalizes the remaining probabilities\n",
    "# Warning: Will not work if the removed vector contains a probability of 1.0\n",
    "def remove_normalize(vector, node_to_remove):\n",
    "    prob = vector.get(node_to_remove)\n",
    "    return { v: 1.0*vector[v]/(1.0-prob) for v in vector if v != node_to_remove}\n",
    "\n",
    "# test\n",
    "def test_normalize():\n",
    "    ta = {\"a\":0.5, \"b\":0.4, \"c\":0.1}\n",
    "    print remove_normalize(ta, \"a\")\n",
    "    \n",
    "test_normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': -1.584962500721156, 'b': 1.584962500721156}\n",
      "{'a': 0.26303440583379367, 'c': -0.3219280948873622, 'b': 0.26303440583379367, 'd': -0.3219280948873622}\n"
     ]
    }
   ],
   "source": [
    "# Helper function. Computes the logodds between the probabilities in two vectors\n",
    "from math import log\n",
    "\n",
    "# Computes the log-odds of two probabilities. \n",
    "# The s is a smoothing factor: With 0 we have no smoothing \n",
    "# (note: s=0 will cause an error if there are events with 0 probability in either vector)\n",
    "# The n is the total number of nodes\n",
    "def logodds(pa, pb, s, n):\n",
    "    pa = 0.0 if pa == None else pa\n",
    "    pb = 0.0 if pb == None else pb\n",
    "    a = (pa + s) / ( 1.0 + s*n)\n",
    "    b = (pb + s) / ( 1.0 + s*n)\n",
    "    return  log(a,2) - log(b,2)\n",
    "\n",
    "def logodds_vector(vector_a, vector_b, s):\n",
    "    keys = set(vector_a.keys()) | set(vector_b.keys())\n",
    "    n = len(keys)\n",
    "    return { k: logodds(vector_a.get(k), vector_b.get(k), s, n) for k in keys}\n",
    "\n",
    "# test\n",
    "def test_logodds():\n",
    "    ta = {\"a\":0.25, \"b\":0.75}\n",
    "    tb = {\"a\":0.75, \"b\":0.25}\n",
    "    print logodds_vector(ta, tb, 0)\n",
    "\n",
    "    ta = {\"a\":0.5, \"b\":0.5}\n",
    "    tb = {\"a\":0.25, \"b\":0.25, \"c\":0.25, \"d\": 0.25}\n",
    "    print logodds_vector(ta, tb, 1)\n",
    "    \n",
    "\n",
    "test_logodds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': {'a': 0.5510992275698159,\n",
      "       'b': 0.00326797385620915,\n",
      "       'c': 0.44563279857397503},\n",
      " 'b': {'b': 0.5413223140495869,\n",
      "       'c': 0.4132231404958677,\n",
      "       'd': 0.045454545454545456},\n",
      " 'c': {'a': 0.30864197530864196,\n",
      "       'b': 0.2777777777777778,\n",
      "       'c': 0.24691358024691368,\n",
      "       'd': 0.16666666666666666},\n",
      " 'd': {'a': 0.002849002849002849,\n",
      "       'c': 0.08741258741258742,\n",
      "       'd': 0.9097384097384097}}\n"
     ]
    }
   ],
   "source": [
    "# The Good Vibes model defines the probability of a vibe being noticed as \n",
    "# i->j = count(i,j)/sum(count(i,*)) * count(i,j)/sum(count(*,j))\n",
    "# We represent the adjacency matrix as a dictionary of dictionaries\n",
    "# i: {j: count(i,j)/sum(count(i,*)) * count(i,j)/sum(count(*,j)) }\n",
    "def get_good_vibes_adjacency_matrix(msg_counts):\n",
    "    \n",
    "    # We first compute the from/to normalizing values for each node\n",
    "    to_counts = dict()\n",
    "    from_counts = dict()\n",
    "    for n,m,c in msg_counts:\n",
    "        \n",
    "        # We will ignore self-sending messages\n",
    "        #if (n==m):\n",
    "        #    continue\n",
    "        \n",
    "        count_n = to_counts.get(n)\n",
    "        if count_n == None:\n",
    "            count_n = 0.0\n",
    "        to_counts[n] = c + count_n\n",
    "    \n",
    "        count_m = from_counts.get(m)\n",
    "        if count_m == None:\n",
    "            count_m = 0.0\n",
    "        from_counts[m] = c + count_m\n",
    "\n",
    "    # We compute the weights using the Good Vibes formula\n",
    "    adjacency_matrix = dict()\n",
    "    for n,m,c in msg_counts:\n",
    "        edges_n = adjacency_matrix.get(n)\n",
    "        if edges_n == None:\n",
    "            edges_n = dict()\n",
    "        edges_n[m] = (1.0*c/to_counts[n]) * (1.0*c/from_counts[m])\n",
    "        adjacency_matrix[n] = edges_n\n",
    "    \n",
    "    # We assigned the unallocated probability into a self-loop\n",
    "    for n, edges in adjacency_matrix.iteritems():\n",
    "        total_n = sum(edges.values())\n",
    "        edges[n] = 1-total_n\n",
    "        adjacency_matrix[n] = edges\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "\n",
    "import pprint \n",
    "def test_gv():\n",
    "\n",
    "    message_counts = [\n",
    "        (\"a\", \"b\", 50),\n",
    "        (\"a\", \"c\", 2500),\n",
    "        (\"b\", \"c\", 2500),\n",
    "        (\"b\", \"d\", 250),\n",
    "        (\"c\", \"a\", 250),\n",
    "        (\"c\", \"b\", 250),\n",
    "        (\"c\", \"d\", 250),\n",
    "        (\"d\", \"c\", 500),\n",
    "        (\"d\", \"a\", 20),\n",
    "    ]\n",
    "    \n",
    "    adj = get_good_vibes_adjacency_matrix(message_counts)\n",
    "    \n",
    "    pprint.pprint(adj)\n",
    "\n",
    "test_gv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# node_probs is a dictionary { \"node\": probability, ...} with the probabilities on round k\n",
    "# revised_probs is a dictionary { \"node\": probability, ... } with the probabilities on round k+1\n",
    "# for the structure of the adjacency matrix, look above \n",
    "def propagate(node_probs, adjacency, smoothing_probs, d):\n",
    "    revised_probs = dict()\n",
    "    for n, p_n in node_probs.iteritems():\n",
    "        # The edge_probs is a dictionary { \"node\": probability, ...} \n",
    "        # that contains the probability that a vibe is propagated\n",
    "        # from n to m\n",
    "        propagation_probs_from_n = adjacency.get(n)\n",
    "        if propagation_probs_from_n == None:\n",
    "            continue\n",
    "        for m, p_n_m in propagation_probs_from_n.iteritems():\n",
    "            p_m = revised_probs.get(m)\n",
    "            if p_m == None:\n",
    "                p_m = 0\n",
    "            p_m += p_n * p_n_m\n",
    "            revised_probs[m] = p_m\n",
    "    \n",
    "    return add_vectors(revised_probs, smoothing_probs, 1-d, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computePagerank(adjacency, d, max_iterations, max_diff):\n",
    "    nodes = set(adjacency.keys())\n",
    "    smoothing = dict()\n",
    "    for n in nodes:\n",
    "        smoothing[n] = 1.0/len(nodes) \n",
    "    pagerank = smoothing\n",
    "    \n",
    "    for i in range(0,max_iterations):\n",
    "        pagerank_new = propagate(pagerank, adjacency, smoothing, d)\n",
    "        # print pagerank_new\n",
    "        # check for convergence\n",
    "        diff = sum(diff_vectors(pagerank_new, pagerank).values())\n",
    "        if diff < max_diff:\n",
    "            break\n",
    "        else:\n",
    "            pagerank = pagerank_new\n",
    "        \n",
    "    return pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computePersonalizedPagerank(adjacency, start, d, max_iterations, max_diff):\n",
    "    \n",
    "    ppr = {start:1.0}\n",
    "    smoothing_probs = {start:1.0}\n",
    "    for i in range(0,max_iterations):\n",
    "        ppr_new = propagate(ppr, adjacency, smoothing_probs, d)\n",
    "        # print ppr_new\n",
    "        # check for convergence\n",
    "        diff = sum(diff_vectors(ppr_new, ppr).values())\n",
    "        if diff < max_diff:\n",
    "            break\n",
    "        else:\n",
    "            ppr = ppr_new\n",
    "    \n",
    "    return ppr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_matrix(filename):\n",
    "    f = open(\"data/msg_counts.csv\", \"r\")\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    \n",
    "    msg_counts = []\n",
    "    # skip the header line and the last line (because it is empty, and I am lazy to remove it)\n",
    "    for line in lines[1: len(lines)-2]:\n",
    "        fields = line.split(\"\\t\")\n",
    "        # Tuple = sender, recipient, count\n",
    "        t = (str(int(fields[0])), str(int(fields[1])), float(fields[2]) )\n",
    "        msg_counts.append(t)\n",
    "    return get_good_vibes_adjacency_matrix(msg_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERSONALIZED PAGERANK for a \n",
      "{'a': 0.8, 'c': 0.03196799999999999, 'b': 0.053279999999999994, 'e': 0.0015983999999999987, 'd': 0.006393599999999997, 'g': 0.053279999999999994, 'f': 0.053279999999999994}\n",
      "RENORMALIZED PERSONALIZED PAGERANK for a \n",
      "{'c': 0.15983999999999998, 'b': 0.2664, 'e': 0.007991999999999996, 'd': 0.03196799999999999, 'g': 0.2664, 'f': 0.2664}\n"
     ]
    }
   ],
   "source": [
    "# This is a test adjacency matrix,\n",
    "#adjacency_matrix = {\"a\": {\"b\": 0.5, \"c\":0.25, \"a\":0.25}, \n",
    "#              \"b\": {\"c\": 0.25, \"d\":0.25, \"b\": 0.5}, \n",
    "#              \"c\": {\"a\": 0.25, \"b\": 0.25, \"d\": 0.25, \"c\": 0.25},\n",
    "#              \"d\": {\"c\":0.25, \"a\":0.1, \"d\": 0.65}\n",
    "#             }\n",
    "\n",
    "adjacency_matrix = {\"a\": {\"b\": 0.333, \"f\": 0.333, \"g\": 0.333}, \n",
    "              \"b\": {\"c\": 1.0}, \n",
    "              \"c\": {\"d\": 1.0},\n",
    "              \"d\": {\"e\": 1.0},\n",
    "              \"e\": {\"e\": 1.0},\n",
    "              \"f\": {\"c\": 1.0},\n",
    "              \"g\": {\"c\": 1.0}  \n",
    "             }\n",
    "\n",
    "d = 0.8\n",
    "max_iterations = 30\n",
    "max_diff = 0.00001\n",
    "start_node = \"a\"\n",
    "\n",
    "# Computes the personalized pagerank of start_node\n",
    "ppr = computePersonalizedPagerank(adjacency_matrix, start_node, d, max_iterations, max_diff)\n",
    "print \"PERSONALIZED PAGERANK for\", start_node, \"\\n\", ppr\n",
    "norm_ppr = remove_normalize(ppr, start_node)\n",
    "print \"RENORMALIZED PERSONALIZED PAGERANK for\", start_node, \"\\n\", norm_ppr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21950 0.9903\n",
      "37821 0.0152\n"
     ]
    }
   ],
   "source": [
    "def testpagerank(): \n",
    "    \n",
    "    filename = \"data/msg_counts.csv\"\n",
    "    adjacency_matrix = load_matrix(filename)\n",
    "\n",
    "    d = 0.7\n",
    "    max_iterations = 30\n",
    "    max_diff = 0.0000000001\n",
    "    start_node = \"33618\"\n",
    "    # start_node = \"021950\"\n",
    "\n",
    "    # Computes the pagerank of each node\n",
    "    # Should be run just once, as it does not change\n",
    "    pr = computePagerank(adjacency_matrix, d, max_iterations, max_diff)\n",
    "    # print \"PAGERANK\\n\", pr  \n",
    "\n",
    "    # Computes the personalized pagerank of start_node\n",
    "    ppr = computePersonalizedPagerank(adjacency_matrix, start_node, d, max_iterations, max_diff)\n",
    "    # print \"PERSONALIZED PAGERANK for\", start_node, \"\\n\", ppr\n",
    "    \n",
    "    # Since we want to compute the connections of start_node, we renormalize the vector\n",
    "    # of probabilities to exclude the start_node, and allocate the probability proportionally\n",
    "    # to the other nodes. Now we have two sets of probabilities. One is from general pagerank\n",
    "    # which captures the prominence of the node in the network, and one is from personalized\n",
    "    # pagerank which illustrates the prominence of each node wrt to start_node\n",
    "    norm_pr = remove_normalize(pr, start_node)\n",
    "    norm_ppr = remove_normalize(ppr, start_node)\n",
    "    # print \"RENORMALIZED PAGERANK after removing\", start_node, \"\\n\", norm_pr\n",
    "    # print \"RENORMALIZED PERSONALIZED PAGERANK for\", start_node, \"\\n\", norm_ppr\n",
    "    \n",
    "    \n",
    "    # We now compute the log-odds of each node. Positive values mean higher association\n",
    "    # with the start node compared to the expectation (Value of 1 means \"twice as likely compared to\n",
    "    # expectation\", value 2 means four times as likely, etc.)\n",
    "    log_odds = logodds_vector(norm_ppr, norm_pr, 1)\n",
    "    for k, v in log_odds.iteritems():\n",
    "        # We print only nodes with \"significantly\" positive scores (\"significantly\" above 0.0)\n",
    "        if v>0.01:\n",
    "            print k, round(v,4)\n",
    "\n",
    "    # print \"LOG-ODDS score for\", start_node, \"\\n\",log_odds\n",
    "    \n",
    "testpagerank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
